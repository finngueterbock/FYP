{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n"],"metadata":{"id":"tXvZUyN1Cnz9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_60uZSwa-Mw-"},"source":["### Imports (run all)"]},{"cell_type":"code","source":["import os\n","os.chdir('drive/MyDrive/stellargraph') # edited stellargraph requirments to work with python 3.9\n","%pip install .\n","os.chdir('..')"],"metadata":{"id":"0iMhH0Uc3HEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install kaleido "],"metadata":{"id":"rfyMLa5ZCMXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sAtc_Oc7-MxC"},"outputs":[],"source":["import kaleido\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","import json\n","from tqdm.auto import tqdm\n","import ast\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error\n","import random\n","import plotly.graph_objects as go\n","import pickle\n","from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization, Conv1D, MaxPooling1D, Flatten\n","import networkx as nx\n","from stellargraph.mapper import GraphSAGENodeGenerator\n","from stellargraph.layer import GraphSAGE, MeanAggregator, MaxPoolingAggregator, MeanPoolingAggregator, AttentionalAggregator\n","from stellargraph import StellarGraph\n","import stellargraph as sg\n","from tensorflow.keras import layers, optimizers, losses, metrics, Model, initializers\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","import warnings\n","from math import radians, sin, cos, sqrt, atan2\n","import gc\n","import itertools"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1yCmukT9-MxE"},"outputs":[],"source":["def nice_plot(fig,  x_label=None, y_label=None, title=None, height=550, width=800, legend=True, y_range=None, x_range=None):\n","    # set background to white\n","    fig.update_layout(plot_bgcolor='white')\n","    # change fig size\n","    fig.update_layout(height=height, width=width)\n","    # change x axis title\n","    if x_label:\n","        fig.update_xaxes(title_text=x_label)\n","    #change y axis title\n","    if y_label:\n","        fig.update_yaxes(title_text=y_label)\n","\n","    # change title\n","    if title:\n","        fig.update_layout(title_text=title, title_x=0.5)\n","\n","    if not legend:\n","        fig.update_layout(showlegend=False)\n","\n","    if y_range:\n","        fig.update_layout(yaxis_range=y_range)\n","        \n","    # fig.update_layout(\n","    #     margin=dict(l=0, r=0, b=0, t=0),\n","    # )\n","    \n","    # change font to latex font\n","    fig.update_layout(font_family='serif')\n","    fig.update_layout(font_size=20)\n","    \n","    # make font color black\n","    fig.update_layout(font_color='black')\n","    \n","    # add axis lines\n","    fig.update_yaxes(showline=True,  # add line at x=0\n","                     linecolor='black',  # line color\n","                     linewidth=2.4,  # line size\n","                     #  ticks='outside',  # ticks outside axis\n","                     mirror='allticks',  # add ticks to top/right axes\n","                     tickwidth=2.4,  # tick width\n","                     tickcolor='black',  # tick color\n","                     )\n","    fig.update_xaxes(showline=True,\n","                     showticklabels=True,\n","                     linecolor='black',\n","                     linewidth=2.4,\n","                     #  ticks='outside',\n","                     mirror='allticks',\n","                     tickwidth=2.4,\n","                     tickcolor='black',\n","                     )\n","    return fig\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtyK_BQI-MxF"},"outputs":[],"source":["bristol = pd.read_feather('bristol_ground_sat_weather 2.feather')\n","\n","with open('lat_long_to_location.pkl', 'rb') as f:\n","    lat_long_to_location = pickle.load(f)\n","bristol.lat_long = bristol.lat_long.apply(lambda x: tuple(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eUKPm1Pu-MxG"},"outputs":[],"source":["london = pd.read_feather(path='london_ground_sat_weather.feather')\n","london.lat_long = london.lat_long.apply(lambda x: tuple(x))"]},{"cell_type":"markdown","metadata":{"id":"uuskjXGf-MxH"},"source":["### pick df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJLWHVUO-MxH"},"outputs":[],"source":["df = bristol.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4yh56j--MxI"},"outputs":[],"source":["# df = london.copy()"]},{"cell_type":"markdown","metadata":{"id":"-KaXvBCk-MxJ"},"source":["### normalise data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xRnZcOkw-MxJ"},"outputs":[],"source":["df['absorbing_aerosol_index'] = (df['absorbing_aerosol_index'] - df['absorbing_aerosol_index'].mean()) / df['absorbing_aerosol_index'].std()\n","df['tropospheric_NO2_column_number_density'] = (df['tropospheric_NO2_column_number_density'] - df['tropospheric_NO2_column_number_density'].mean()) / df['tropospheric_NO2_column_number_density'].std()\n","df['temperature_2m'] = (df['temperature_2m'] - df['temperature_2m'].mean()) / df['temperature_2m'].std()\n","df['relativehumidity_2m'] = (df['relativehumidity_2m'] - df['relativehumidity_2m'].mean()) / df['relativehumidity_2m'].std()\n","df['windspeed_10m'] = (df['windspeed_10m'] - df['windspeed_10m'].mean()) / df['windspeed_10m'].std()\n","df['distance_to_road'] = (df['distance_to_road'] - df['distance_to_road'].mean()) / df['distance_to_road'].std()\n","df['dewpoint_2m'] = (df['dewpoint_2m'] - df['dewpoint_2m'].mean()) / df['dewpoint_2m'].std()\n","df['surface_pressure'] = (df['surface_pressure'] - df['surface_pressure'].mean()) / df['surface_pressure'].std()\n","df['cloudcover_low'] = (df['cloudcover_low'] - df['cloudcover_low'].mean()) / df['cloudcover_low'].std()\n","df['winddirection_10m'] = (df['winddirection_10m'] - df['winddirection_10m'].mean()) / df['winddirection_10m'].std()\n","df['windgusts_10m'] = (df['windgusts_10m'] - df['windgusts_10m'].mean()) / df['windgusts_10m'].std()\n","df['vapor_pressure_deficit'] = (df['vapor_pressure_deficit'] - df['vapor_pressure_deficit'].mean()) / df['vapor_pressure_deficit'].std()\n","df['time_of_day'] = df.time.dt.hour / 24 + df.time.dt.minute / (24 * 60) + df.time.dt.second / (24 * 60 * 60)\n","df['lat_long'] = df.lat_long.apply(lambda x: tuple(x))\n","df['day_of_week'] = df.time.dt.day_name()\n","df['day'] = df.time.dt.dayofyear /366\n","df['week'] = df.time.dt.week /53\n","df['lat'] = df.lat_long.apply(lambda x: x[0]).astype(float)\n","df['long'] = df.lat_long.apply(lambda x: x[1]).astype(float)\n","df['lat'] = (df['lat'] - df['lat'].mean()) / df['lat'].std()\n","df['long'] = (df['long'] - df['long'].mean()) / df['long'].std()\n","# ONE HOT ENCODE DAT BITCH\n","df = pd.concat([df, pd.get_dummies(df['day_of_week'])], axis=1)\n","\n","# Sort the dataframe by time\n","df = df.sort_values('time')\n","\n","# Create a new column that contains the NO2 value for that lat_long at the previous timestep\n","df['prev_no2'] = df.groupby('lat_long')['NO2'].shift(1)\n","df['prev_no2'] = df.groupby(['day', 'week', 'time_of_day'])['prev_no2'].transform(lambda x: x.fillna(x.mean()))"]},{"cell_type":"markdown","metadata":{"id":"ThPY0BlX-MxK"},"source":["### define features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vPZtFLRF-MxK"},"outputs":[],"source":["features = ['day', 'week',\n","            'tropospheric_NO2_column_number_density', 'absorbing_aerosol_index',\n","            'temperature_2m', 'relativehumidity_2m', 'windspeed_10m', 'dewpoint_2m',\n","            'surface_pressure', 'cloudcover_low', 'winddirection_10m',\n","            'windgusts_10m', 'vapor_pressure_deficit',\n","            'distance_to_road', 'time_of_day', 'lat', 'long', 'Friday', 'Monday',\n","            'Saturday', 'Sunday', 'Thursday', 'Tuesday', 'Wednesday', 'prev_no2'\n","            ]\n","target = 'NO2'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l2putawV-MxK"},"outputs":[],"source":["def distance(lat_long1, lat_long2):\n","    # Convert to radians\n","    lat1, lon1 = lat_long1\n","    lat2, lon2 = lat_long2\n","    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n","\n","    # Haversine formula\n","    dlon = lon2 - lon1\n","    dlat = lat2 - lat1\n","    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n","    c = 2 * atan2(sqrt(a), sqrt(1-a))\n","    distance = 6371000 * c  # Radius of Earth in meters\n","    return distance\n","\n","def get_tuple(string):\n","    start_index = string.index(\"(\") + 1\n","    end_index = string.index(\")\")\n","    result = string[start_index:end_index]\n","    return tuple(map(float, result.split(\",\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W5ndARe2-MxL"},"outputs":[],"source":["df['features'] = df[features].values.tolist()\n","\n","# remove marlborough streeet from df as data is weird\n","df = df[df.lat_long != (51.459142, -2.595433)]\n","lat_longs = list(df.lat_long.unique())"]},{"cell_type":"markdown","source":["### Define graph structure"],"metadata":{"id":"-qTGoMjs2wgd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RyCdGlri-MxL"},"outputs":[],"source":["timesteps = list(df.time.unique())\n","\n","def get_graph(timestep, n):\n","    df_timestep = df[df.time == timestep]\n","    lat_longs = list(df_timestep.lat_long.unique())\n","    G = nx.Graph()\n","    # create node names, string of lat long, with time step added on\n","    nodes = [str(lat_long) +'-' + str(n) for lat_long in lat_longs]\n","    G.add_nodes_from(nodes)\n","    \n","    for node, other_node in itertools.combinations(nodes, 2):\n","        dis = distance(get_tuple(node), get_tuple(other_node))\n","        if dis < 3900:  \n","            G.add_edge(node, other_node, weight=dis)\n","                \n","    return G\n","\n","def get_graphs(timesteps):\n","    return [get_graph(timestep, n) for n, timestep in enumerate(tqdm(timesteps))]\n","\n","graphs = get_graphs(timesteps)\n","\n","\n","# add in node features and NO2\n","df_dict = {}\n","for _, row in df.iterrows():\n","    time, lat_long, NO2, features = row['time'], row['lat_long'], row['NO2'], row['features']\n","    if time not in df_dict:\n","        df_dict[time] = {}\n","    df_dict[time][lat_long] = {'NO2': NO2, 'features': features}\n","\n","for timestep, graph in tqdm(zip(timesteps, graphs)):\n","    timestep_dict = df_dict.get(timestep, {})\n","    for node in graph.nodes:\n","        node_tuple = get_tuple(node)\n","        node_dict = timestep_dict.get(node_tuple, {})\n","        graph.nodes[node]['NO2'] = node_dict.get('NO2', None)\n","        graph.nodes[node]['lat_long'] = node_tuple\n","        graph.nodes[node]['features'] = node_dict.get('features', None)\n","\n","# create dict of node ids and features\n","\n","all_nodes = {}\n","all_nodes_targets = {}\n","for graph in tqdm(graphs):\n","    for node in graph.nodes():\n","        all_nodes[node] = graph.nodes[node]['features']\n","        all_nodes_targets[node] = graph.nodes[node][target]\n","\n","# convert to dataframe\n","all_nodes_df = pd.DataFrame.from_dict(all_nodes, orient='index')\n","gc.collect()\n","\n","edge_df = nx.to_pandas_edgelist(graphs[0], source='source', target='target')\n","for graph in tqdm(graphs[1:]):\n","    edge_df = pd.concat([edge_df, nx.to_pandas_edgelist(graph, source='source', target='target')])\n","  \n","\n","edge_df.reset_index(drop=True, inplace=True)\n","graph = StellarGraph(nodes=all_nodes_df, edges=edge_df)"]},{"cell_type":"code","source":["# some convenience bits\n","\n","numbers = list(range(df.time.nunique()))\n","timesteps = dict(zip(list(df.sort_values('time').time.unique()), numbers))\n","df['node_name'] = df.apply(lambda x: str(x.lat_long)+ '-' + str(timesteps[x.time]), axis=1)\n","all_nodes_targets = dict(zip(df.node_name, df.NO2))\n","timesteps_dict = {v: k for k, v in timesteps.items()}"],"metadata":{"id":"d0-gD9Rus2IJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train model on graph without test location, then predict on full graph one node at a time (takes a LONG time to run predictions)"],"metadata":{"id":"QtTq-TsC2_fu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MAngBnr_-MxP"},"outputs":[],"source":["warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=DeprecationWarning)\n","nodes = list(graph.nodes())\n","batch_size = 50\n","num_samples = [3, 5]\n","layer_sizes = [64, 64]\n","dropout = 0.5\n","learning_rate = 0.001 \n","aggregator = MeanAggregator\n","epochs = 10\n","all_predictions = {}\n","all_targets = {}\n","predictions = {}\n","all_targets = {}\n","predictions_dfs = {}\n","random.seed(42)\n","tf.random.set_seed(42)\n","np.random.seed(42)\n","\n","test_lat_longs = [(51.427864, -2.563742)]\n","\n","for lat_long in test_lat_longs:\n","  test_lat_longs = [lat_long]\n","  train_lat_longs = [(lat, long) for lat, long in lat_longs if (\n","      lat, long) not in test_lat_longs]\n","\n","  test_nodes = test_lat_longs\n","\n","  test_nodes = [node for node in nodes if ((get_tuple(node) in test_nodes))]\n","  train_nodes = list(set(nodes) - set(test_nodes))\n","\n","  # remove all nodes for testing lat longs\n","  subgraph = graph.subgraph(train_nodes)\n","\n","  test_labels = {node: all_nodes_targets[node] for node in test_nodes}\n","  train_labels = {node: all_nodes_targets[node] for node in train_nodes}\n","\n","  generator = GraphSAGENodeGenerator(subgraph, batch_size, num_samples)\n","\n","  # create numpy array from values of dict\n","  train_targets = np.array(list(train_labels.values()))\n","  test_targets = np.array(list(test_labels.values()))\n","  train_gen = generator.flow(train_nodes, targets=train_targets)\n","\n","\n","  graphsage_model = GraphSAGE(\n","      layer_sizes=layer_sizes, generator=generator, bias=True, dropout=dropout, aggregator=aggregator, kernel_initializer=initializers.glorot_uniform(seed=0)\n","  )\n","\n","  x_inp, x_out = graphsage_model.in_out_tensors()\n","  prediction = layers.Dense(units=1, activation=\"relu\",\n","                            kernel_initializer=initializers.glorot_uniform(seed=0))(x_out)\n","\n","  # compile model\n","\n","  model = Model(inputs=x_inp, outputs=prediction)\n","\n","  model.compile(\n","      optimizer=optimizers.Adam(learning_rate=learning_rate),\n","      loss=losses.mean_squared_error,\n","      metrics=[metrics.mean_absolute_error,\n","              metrics.mean_absolute_percentage_error],\n","  )\n","\n","# train the model\n","\n","  history = model.fit(\n","      train_gen,\n","      epochs=epochs,\n","      verbose=1,\n","      shuffle=False)\n","  \n","  test_targets_dict = dict(zip(test_nodes, test_targets))\n","  \n","\n","  # add test nodes back in\n","  new_gen = GraphSAGENodeGenerator(graph, batch_size, num_samples)\n","  new_graph = graph\n","  test_all_nodes_df = all_nodes_df.copy()\n","\n","  for timestep in tqdm(list(timesteps.values())):\n","      # get the node for that timestep\n","      new_nodes = [node for node in test_nodes if int(\n","          node.split(\"-\")[-1]) == timestep]\n","      if len(new_nodes) == 0:\n","          continue\n","      targets = [test_targets_dict[node] for node in new_nodes]\n","      # get the generator flow for that timestep\n","      test_gen = new_gen.flow(new_nodes, targets=targets)\n","      del new_gen\n","      # predict on that timestep\n","      test_predictions = model.predict(test_gen, verbose=0).flatten()\n","      \n","      # save predictions for all test nodes\n","      for i, node in enumerate(new_nodes):\n","          predictions[node] = test_predictions[i]\n","          # all_targets[node] = test_targets[i]\n","      # change node attributes in all_nodes_df and make new graph then create new generator\n","      test_all_nodes_df.loc[new_nodes].iloc[:, -1] = test_predictions\n","      new_graph = StellarGraph(nodes=test_all_nodes_df, edges=edge_df)\n","      new_gen = GraphSAGENodeGenerator(new_graph, batch_size, num_samples)\n","      del new_graph,  test_gen, test_predictions\n","      if timestep % 500 == 10: # save in case of colab runtime timeout\n","          pd.DataFrame(list(predictions.items()), columns=['key', 'value']).to_csv(\n","              'test_predictions.csv', index=False)\n","      if timestep % 50 == 0:\n","          K. clear_session() # needed to stop Keras memory leak\n","          gc.collect()\n","\n","\n","  target_df = pd.DataFrame.from_dict(all_nodes_targets, orient='index')\n","  target_df.reset_index(inplace=True)\n","  target_df.columns = ['key', 'actual']\n","  pred = pd.DataFrame(list(predictions.items()), columns=['key', 'value'])\n","  pred['time'] = pred['key'].apply(\n","      lambda x: timesteps_dict[int(x.split(\"-\")[-1])])\n","  pred['lat_long'] = pred['key'].apply(get_tuple)\n","  pred = pd.merge(pred, target_df, how='left', on='key')\n","  pred.columns = ['node', 'prediction', 'time', 'lat_long', 'actual']\n","  pred.to_csv('predictions.csv', index=False)\n","\n","\n"]},{"cell_type":"markdown","source":["### PLOT"],"metadata":{"id":"SfzXQrIi3R_p"}},{"cell_type":"code","source":["\n","#  plot predictions vs actuals for each lat_long\n","fig = nice_plot(go.Figure(), title='Predictions vs actuals for each location',\n","                x_label='Time', y_label='NO2', width=1000, height=400)\n","for lat_long in pred['lat_long'].unique():\n","    fig.add_trace(go.Scatter(x=pred[pred['lat_long'] == lat_long]['time'],\n","                             y=pred[pred['lat_long']\n","                                           == lat_long]['actual'],\n","                             mode='lines', name=f'{lat_long_to_location[lat_long]} actual', opacity=0.4))\n","    fig.add_trace(go.Scatter(x=pred[pred['lat_long'] == lat_long]['time'],\n","                             y=pred[pred['lat_long']\n","                                           == lat_long]['prediction'],\n","                             mode='lines', name=f'{lat_long_to_location[lat_long]} prediction'))\n","\n","fig.show()"],"metadata":{"id":"YzfafFGfrtYj"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"orig_nbformat":4,"colab":{"provenance":[],"collapsed_sections":["_60uZSwa-Mw-","-KaXvBCk-MxJ","ThPY0BlX-MxK"],"machine_shape":"hm"},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}