{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpefGybouqNp"
      },
      "source": [
        "## Imports (run all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmvXB6XJAIQs"
      },
      "outputs": [],
      "source": [
        "import kaleido\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import requests as rq\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import ast\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import random\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "# from get_satellite_data import get_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSG60asezDcK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def nice_plot(fig,  x_label, y_label, title=None, height=550, width=800, legend=True, y_range=None, x_range=None):\n",
        "    # set background to white\n",
        "    fig.update_layout(plot_bgcolor='white')\n",
        "    # change fig size\n",
        "    fig.update_layout(height=height, width=width)\n",
        "    # change x axis title\n",
        "    fig.update_xaxes(title_text=x_label)\n",
        "    #change y axis title\n",
        "    fig.update_yaxes(title_text=y_label)\n",
        "\n",
        "    # change title\n",
        "    if title:\n",
        "        fig.update_layout(title_text=title, title_x=0.5)\n",
        "\n",
        "    if not legend:\n",
        "        fig.update_layout(showlegend=False)\n",
        "\n",
        "    if y_range:\n",
        "        fig.update_layout(yaxis_range=y_range)\n",
        "        \n",
        "    fig.update_layout(\n",
        "        margin=dict(l=0, r=0, b=0, t=0),\n",
        "    )\n",
        "    \n",
        "    # add axis lines\n",
        "    fig.update_yaxes(showline=True,  # add line at x=0\n",
        "                     linecolor='black',  # line color\n",
        "                     linewidth=2.4,  # line size\n",
        "                     #  ticks='outside',  # ticks outside axis\n",
        "                     mirror='allticks',  # add ticks to top/right axes\n",
        "                     tickwidth=2.4,  # tick width\n",
        "                     tickcolor='black',  # tick color\n",
        "                     )\n",
        "    fig.update_xaxes(showline=True,\n",
        "                     showticklabels=True,\n",
        "                     linecolor='black',\n",
        "                     linewidth=2.4,\n",
        "                     #  ticks='outside',\n",
        "                     mirror='allticks',\n",
        "                     tickwidth=2.4,\n",
        "                     tickcolor='black',\n",
        "                     )\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3yxQ44zuyAd"
      },
      "source": [
        "## traffic data (not needed for now)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9lJphu-AIQx"
      },
      "outputs": [],
      "source": [
        "\n",
        "traffic = pd.read_feather('london_traffic.feather')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "d9eqDkPGR8Mn",
        "outputId": "da2126b0-9a12-431b-8b21-7c183ab0e2ec"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISYdM7MnCQs7"
      },
      "outputs": [],
      "source": [
        "# change to ndarray to list\n",
        "traffic.lat_long = traffic.lat_long.apply(lambda x: x.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyYhYugUr0dX"
      },
      "outputs": [],
      "source": [
        "traffic['sum'] = traffic.lat_long.apply(lambda x: sum(x))\n",
        "sums = list(traffic['sum'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHIZzIiosBiL"
      },
      "outputs": [],
      "source": [
        "unique_sums = traffic['sum'].unique()\n",
        "lon_lat_longs = traffic[traffic['sum'].isin(unique_sums)].lat_long.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHuP2NRrzAtv"
      },
      "outputs": [],
      "source": [
        "traffic.strings = traffic.lat_long.astype(str)\n",
        "unique_strings = traffic.strings.unique()\n",
        "lat_longs = [ast.literal_eval(x) for x in unique_strings]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7tJIXZk02zO"
      },
      "outputs": [],
      "source": [
        "\n",
        "lats = [x[0] for x in lat_longs]\n",
        "longs = [x[1] for x in lat_longs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSBmr9wZNKJB"
      },
      "outputs": [],
      "source": [
        "lon_lat_longs = traffic.lat_long.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPVEE-lMN40P"
      },
      "outputs": [],
      "source": [
        "lon_lat_longs_1 = lon_lat_longs[::2]\n",
        "lon_lat_longs_2 = lon_lat_longs[1::2]\n",
        "lon_lat_longs = list(zip(lon_lat_longs_1, lon_lat_longs_2)) # all the unique UK long and lats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCKwUT-tmQXs"
      },
      "outputs": [],
      "source": [
        "# plot lon_lat_longs on map\n",
        "fig = px.scatter_mapbox(lat=lats, lon=longs, mapbox_style='carto-positron')\n",
        "fig.update_layout(width=500, height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a8FQE-qz83c"
      },
      "outputs": [],
      "source": [
        "traffic.drop(columns=['level_0', 'index', 'id'], inplace=True)\n",
        "traffic['lat_long'] = traffic.lat_long.apply(lambda x: tuple(x.tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfD6qaaV1yb9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def haversine(lat_long1, lat_long2):\n",
        "    R = 6371 # radius of earth in kilometers\n",
        "    lat1, lon1 = lat_long1\n",
        "    lat2, lon2 = lat_long2\n",
        "    phi1, phi2 = math.radians(lat1), math.radians(lat2)\n",
        "    delta_phi = math.radians(lat2 - lat1)\n",
        "    delta_lambda = math.radians(lon2 - lon1)\n",
        "    a = math.sin(delta_phi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda / 2)**2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "    distance = R * c\n",
        "    return distance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pO_cRWRO5qkx",
        "outputId": "9186562f-d3ed-4e97-8fe3-5d3f30da73a9"
      },
      "outputs": [],
      "source": [
        "t = '2020-05-05 19:00:00'\n",
        "lat_long = ground.lat_long.iloc[-1]\n",
        "dists = traffic[traffic['date_time'] == '2021-05-05 07:00:00']['lat_long'].apply(lambda x: haversine(lat_long, x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhRK1r_OAoB5"
      },
      "outputs": [],
      "source": [
        "def distance_between_points(point1, point2):\n",
        "    x1, y1 = point1\n",
        "    x2, y2 = point2\n",
        "    return ((x2 - x1) ** 2 + (y2 - y1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hT0CwEd0_hvO",
        "outputId": "bfa65c24-cabb-4edb-fc1f-493abaea8624"
      },
      "outputs": [],
      "source": [
        "min(traffic[traffic['date_time'] == '2021-05-05 07:00:00']['lat_long'].apply(lambda x: abs(distance_between_points(lat_long, x))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xp1wC-CbCFkd"
      },
      "outputs": [],
      "source": [
        "traffic['strings'] = traffic.lat_long.astype(str)\n",
        "unique_strings = traffic.strings.unique()\n",
        "traffic_points = [ast.literal_eval(x) for x in unique_strings]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHnHvA6RJzS7"
      },
      "outputs": [],
      "source": [
        "traffic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daMaLqYPNy1j",
        "outputId": "682d184e-edd3-48f2-d1e5-6a6d349fd2fc"
      },
      "outputs": [],
      "source": [
        "hours.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAgEzDX7EMBn"
      },
      "outputs": [],
      "source": [
        "traffic['date'] = traffic.date_time.dt.date\n",
        "hours = traffic.groupby('date')['count'].count()\n",
        "px.scatter(hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzQtqGD7CUgl"
      },
      "outputs": [],
      "source": [
        "lat1 = [x[0] for x in traffic_points]\n",
        "lon1 = [x[1] for x in traffic_points]\n",
        "\n",
        "lat2 = [x[0] for x in ground_locs]\n",
        "lon2 = [x[1] for x in ground_locs]\n",
        "\n",
        "fig = px.scatter_mapbox(lat=lat1 + lat2, lon=lon1+lon2, \n",
        "                        mapbox_style='carto-positron',\n",
        "                        color=[0 for _ in range(len(lat1))] + [0.5 for _ in range(len(lat2))],\n",
        "                        color_discrete_sequence=['red', 'blue'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPpvebgQDptm"
      },
      "outputs": [],
      "source": [
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ij2a3JpsRP7j"
      },
      "outputs": [],
      "source": [
        "def weight(distance, cutoff):\n",
        "  if distance < 1 :\n",
        "    weighting = 1-distance\n",
        "  else: \n",
        "    weighting = 0\n",
        "  return weight\n",
        "\n",
        "def get_traffic(traffic_df, lat_long, time=None, cutoff=1):\n",
        "\n",
        "  # get traffic locations\n",
        "  traffic_df['strings'] = traffic_df.lat_long.astype(str)\n",
        "  unique_strings = traffic_df.strings.unique()\n",
        "  traffic_points = [ast.literal_eval(x) for x in unique_strings]\n",
        "\n",
        "\n",
        "  # calculate distance of between sensor and all lat_long points\n",
        "  distances = {x:haversine(lat_long, x) for x in traffic_points}\n",
        "  weights = {k:weight(v) for k, v in distances}\n",
        "\n",
        "  \n",
        "\n",
        "  return \n",
        "\n",
        "#\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_bcwJ2sivITO"
      },
      "source": [
        "## Bristol ground data preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ou1KXLumq0GL"
      },
      "source": [
        "### Preprocessing steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo_KtfAJx3iR",
        "outputId": "ff93e9de-9462-4f0e-c974-54bfec60cb78"
      },
      "outputs": [],
      "source": [
        "# load ground-based data\n",
        "from pandas.api.types import CategoricalDtype\n",
        "ground = pd.read_feather('air_quality_data.feather')\n",
        "\n",
        "# convert ground date time to utc\n",
        "ground['time'] = pd.to_datetime(ground['Date Time'], utc=True)\n",
        "\n",
        "# create lat_long column for ground data\n",
        "ground['lat_long'] = ground.geo_point_2d.apply(\n",
        "    lambda x: x.split(',')).apply(lambda x: (float(x[0]), float(x[1])))\n",
        "\n",
        "# round lat_long column to 3 dp\n",
        "ground['lat_long'] = ground.lat_long.apply(\n",
        "    lambda x: (round(x[0], 3), round(x[1], 3)))\n",
        "\n",
        "# get unique points from ground data\n",
        "points = ground.lat_long.unique().tolist()\n",
        "\n",
        "days = ['Monday', 'Tuesday', 'Wednesday',\n",
        "        'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "cat_type = CategoricalDtype(categories=days, ordered=True)\n",
        "ground['day_of_week'] = ground.time.dt.day_name().astype(cat_type)\n",
        "\n",
        "ground['day'] = ground['time'].dt.day\n",
        "ground['week'] = ground['time'].dt.week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-XcRcZIy-s8"
      },
      "outputs": [],
      "source": [
        "ground = ground[ground['Date Time'] > start]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQiICywRAIRC"
      },
      "outputs": [],
      "source": [
        "# combine satellite with ground\n",
        "bris_combined = pd.merge_asof(satellite.sort_values('time'), ground.sort_values('time').dropna(subset=['time']), on='time', by='lat_long', direction='nearest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNz9xj71AIRC"
      },
      "outputs": [],
      "source": [
        "# change type to string\n",
        "bris_combined['Location'] = bris_combined['Location'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQUQDcnbAIRD"
      },
      "outputs": [],
      "source": [
        "bris_combined.to_feather('bristol_ground+sat.feather')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lo7MI-uvt1jN"
      },
      "source": [
        "### Create bristol ground, satellite and combined dataframes (preprocessed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taa7Z4bUAIRA",
        "outputId": "731b1395-5a41-4849-c473-e7c8ebff7b8f"
      },
      "outputs": [],
      "source": [
        "from get_satellite_data import get_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFAW8FEprUPm"
      },
      "outputs": [],
      "source": [
        "start = '2018-07-11'\n",
        "end = '2022-10-31'\n",
        "ground = pd.read_feather('bristol_ground_cleaned.feather')\n",
        "no2_lat_longs = [list(y) for y in list(ground.lat_long.apply(lambda x: tuple(x)).unique())]\n",
        "reverse_no2_lat_longs = [point[::-1] for point in no2_lat_longs]\n",
        "no2 = get_data(type='nitrogen dioxide', location=reverse_no2_lat_longs, start_date=start, end_date=end, scale=1)\n",
        "ground_locs = [ast.literal_eval(x) for x in ground.geo_point_2d.unique()]\n",
        "bris_combined = pd.read_feather('bristol_ground_sat_weather.feather')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# aerosol = get_data(type='aerosol', location=reverse_no2_lat_longs, start_date=start, end_date=end, scale=1)\n",
        "# bris_combined.lat_long = bris_combined.lat_long.apply(lambda x: tuple(x))\n",
        "bristol = pd.merge_asof(aerosol.sort_values('time'), bris_combined.sort_values('time').dropna(subset=['time']), on='time', by='lat_long', direction='nearest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bristol.to_feather('bristol_ground_sat_weather_new.feather')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4cJAtL7uWH1"
      },
      "source": [
        "### Add in weather to create model_df dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HyGXSZxuUTW",
        "outputId": "7ae07d1e-f02f-4a85-e040-6738f3ea13da"
      },
      "outputs": [],
      "source": [
        "model_df = pd.DataFrame(columns=['longitude', 'latitude', 'time',\n",
        "       'tropospheric_NO2_column_number_density', 'lat_long', 'Date Time',\n",
        "       'NOx', 'NO2', 'NO', 'SiteID', 'PM10', 'NVPM10', 'VPM10', 'NVPM2.5',\n",
        "       'PM2.5', 'VPM2.5', 'CO', 'O3', 'SO2', 'Temperature', 'RH',\n",
        "       'Air Pressure', 'Location', 'geo_point_2d', 'DateStart', 'DateEnd',\n",
        "       'Current', 'Instrument Type', 'day_of_week', 'day', 'week',\n",
        "       'temperature_2m', 'relativehumidity_2m', 'windspeed_10m'])\n",
        "\n",
        "for lat, long in bris_combined.lat_long.unique():\n",
        "    print(lat, long)\n",
        "    weather_url = f\"https://archive-api.open-meteo.com/v1/era5?latitude={lat}&longitude={long}&start_date={start}&end_date={end}&hourly=temperature_2m,relativehumidity_2m,windspeed_10m\"\n",
        "    response = rq.get(weather_url)\n",
        "    weather_json = response.json()\n",
        "    weather = pd.DataFrame.from_dict(weather_json['hourly'])\n",
        "    weather['time'] = pd.to_datetime(weather['time'], utc=True)\n",
        "\n",
        "    model_df = pd.concat([model_df, pd.merge_asof(bris_combined[bris_combined.lat_long == (lat, long)].sort_values('time'), weather.sort_values('time'), on='time', direction='nearest', tolerance=pd.Timedelta('30min')).dropna(subset=['NO2'])])\n",
        "\n",
        "model_df.reset_index(inplace=True)\n",
        "model_df.drop(columns=['index'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IU_R1yxHxorW"
      },
      "outputs": [],
      "source": [
        "model_df.to_feather('bristol_ground_sat_weather.feather')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PvBgvPTsyf_q"
      },
      "source": [
        "## Just read in bristol ground, satellite, time and weather dataframe (model_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ywe4xLeybhy"
      },
      "outputs": [],
      "source": [
        "model_df = pd.read_feather('bristol_ground_sat_weather.feather')\n",
        "bristol = pd.read_feather('bristol_ground_sat_weather.feather')\n",
        "\n",
        "ground = pd.read_feather('bristol_ground_cleaned.feather')\n",
        "ground.lat_long = ground.lat_long.apply(lambda x: tuple(x))\n",
        "\n",
        "# get lat_long to location dict\n",
        "# read in pickle\n",
        "with open('lat_long_to_location.pkl', 'rb') as f:\n",
        "    lat_long_to_location = pickle.load(f)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## London ground data preprocessing"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read in London ground data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "london_ground = pd.read_feather('all_london_data.feather')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get locations of London ground stations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = \"http://api.erg.kcl.ac.uk/AirQuality/Information/MonitoringSites/GroupName=London/Json\"\n",
        "response = rq.get(url)\n",
        "data = response.json()\n",
        "\n",
        "locs = pd.DataFrame.from_dict(data['Sites'], orient='columns')\n",
        "\n",
        "# expand the nested dictionary\n",
        "locs = pd.concat([locs.drop(['Site'], axis=1),\n",
        "                 locs['Site'].apply(pd.Series)], axis=1)\n",
        "\n",
        "# remove @ sign from column names\n",
        "locs.columns = locs.columns.str.replace('@', '')\n",
        "\n",
        "# get rid of closed stations\n",
        "locs = locs[locs.DateClosed == ''].reset_index()\n",
        "\n",
        "# drop date closed and index columns, not needed\n",
        "locs.drop(['DateClosed', 'index'], axis=1, inplace=True)\n",
        "locs = locs[locs.Latitude != '']\n",
        "\n",
        "locs['lat_long'] = locs.Latitude.apply(lambda x: round(float(x), 3)).astype(\n",
        "    str) + ',' + locs.Longitude.apply(lambda x: round(float(x), 3)).astype(str)\n",
        "locs['lat_long'] = locs['lat_long'].apply(\n",
        "    lambda x: tuple(map(float, x.split(','))))\n",
        "\n",
        "london_lat_longs = locs.lat_long.unique()\n",
        "reversed_points = [point[::-1] for point in london_lat_longs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "code_to_lat_long = dict(zip(locs.SiteCode, locs.lat_long))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add in satellite data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sat_aerosol = get_data(type='aerosol', location=reversed_points[:35], start_date=start, end_date=end, scale=1)\n",
        "sat_aerosol = sat_aerosol.append(get_data(type='aerosol', location=reversed_points[35:70], start_date=start, end_date=end, scale=1))\n",
        "sat_aerosol = sat_aerosol.append(get_data(type='aerosol', location=reversed_points[70:], start_date=start, end_date=end, scale=1))\n",
        "\n",
        "sat = get_data(type='nitrogen dioxide',location=reversed_points[:35], start_date=start, end_date=end, scale=1)\n",
        "sat = sat.append(get_data(type='nitrogen dioxide',location=reversed_points[35:70], start_date=start, end_date=end, scale=1))\n",
        "sat = sat.append(get_data(type='nitrogen dioxide',location=reversed_points[70:], start_date=start, end_date=end, scale=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merge NO2 and aerosol data\n",
        "london_sat = pd.merge_asof(sat_aerosol.sort_values('time'), sat.sort_values('time'), on='time', by='lat_long',\n",
        "              direction='nearest')[['lat_long', 'time', 'absorbing_aerosol_index', 'tropospheric_NO2_column_number_density']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "london_ground['time'] = london_ground['MeasurementDateGMT']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# change dtype to string, change to datetime, then we can merge\n",
        "\n",
        "london_ground.site_code = london_ground.site_code.astype(str)\n",
        "london_ground['lat_long'] = london_ground['site_code'].map(code_to_lat_long)\n",
        "london_ground.time = pd.to_datetime(london_ground.time, utc=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merge satellite and ground data\n",
        "london = pd.merge_asof(london_ground.sort_values('time'), london_sat.sort_values('time'), on='time', by='lat_long',\n",
        "                direction='nearest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "london.drop(columns=['Nitric Oxide (ug/m3)', \n",
        "             'Oxides of Nitrogen (ug/m3)', 'Sulphur Dioxide (ug/m3)', 'site_code',\n",
        "             'PM10 Particulate (ug/m3)', 'PM2.5 Particulate (ug/m3)',\n",
        "             'Carbon Monoxide (mg/m3)', 'Ozone (ug/m3)'], inplace=True)\n",
        "london.drop('MeasurementDateGMT', axis=1, inplace=True)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add in weather to london ground and satellite data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "london_df = pd.DataFrame(columns=['Nitrogen Dioxide (ug/m3)', 'time', 'lat_long',\n",
        "       'absorbing_aerosol_index', 'tropospheric_NO2_column_number_density',\n",
        "       'temperature_2m', 'relativehumidity_2m', 'windspeed_10m'])\n",
        "\n",
        "i = 0\n",
        "for lat, long in london.lat_long.dropna().unique():\n",
        "    i += 1\n",
        "    print(lat, long, i)\n",
        "    weather_url = f\"https://archive-api.open-meteo.com/v1/era5?latitude={lat}&longitude={long}&start_date={start}&end_date={end}&hourly=temperature_2m,relativehumidity_2m,windspeed_10m\"\n",
        "    response = rq.get(weather_url)\n",
        "    weather_json = response.json()\n",
        "    weather = pd.DataFrame.from_dict(weather_json['hourly'])\n",
        "    weather['time'] = pd.to_datetime(weather['time'], utc=True)\n",
        "\n",
        "    london_df = pd.concat([london_df, pd.merge_asof(london[london.lat_long == (lat, long)].sort_values('time'), weather.sort_values('time'), on='time', direction='nearest', tolerance=pd.Timedelta('30min')).dropna(subset=['Nitrogen Dioxide (ug/m3)'])])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "london_df.reset_index(inplace=True)\n",
        "london_df.drop(columns=['index'], inplace=True)\n",
        "london_df.to_feather(path='london_ground_sat_weather.feather')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Just read in london ground, satellite, time and weather dataframe (london)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "london = pd.read_feather(path='london_ground_sat_weather.feather')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XbZ-d2wyxe_"
      },
      "source": [
        "## Model experimentation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add in extra features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-h8gHAf5DV9"
      },
      "outputs": [],
      "source": [
        "model_df['lat_long'] = model_df.lat_long.apply(lambda x: tuple(x))\n",
        "model_df['hour'] = model_df.time.dt.hour\n",
        "model_df['minute'] = model_df.time.dt.minute\n",
        "\n",
        "# ONE HOT ENCODE DAT BITCH\n",
        "model_df = pd.concat([model_df, pd.get_dummies(model_df['day_of_week'])], axis=1)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create train and test sets "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a list of the lat_longs\n",
        "lat_longs = list(model_df.lat_long.unique())\n",
        "lat_longs.remove((51.459, -2.595))\n",
        "\n",
        "# create a list of 5 random lat_longs to train on\n",
        "train_lat_longs = random.sample(lat_longs, 5)\n",
        "\n",
        "# create a list of the remaining lat_longs to test on\n",
        "test_lat_longs = [x for x in lat_longs if x not in train_lat_longs]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### code to test for all locations and plot on map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mses = {}\n",
        "# train on all but one lat_long\n",
        "for loc in tqdm(lat_longs):\n",
        "    location = lat_long_to_location[loc]\n",
        "    test_lat_longs = [loc]\n",
        "    train_lat_longs = [x for x in lat_longs if x not in test_lat_longs]\n",
        "    \n",
        "    # create a list of the features\n",
        "    features = ['hour', 'minute',\n",
        "                'tropospheric_NO2_column_number_density',\n",
        "                'NO2',\n",
        "                'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday',\n",
        "                'day', 'week',\n",
        "                'temperature_2m', 'relativehumidity_2m', 'windspeed_10m'\n",
        "                ]\n",
        "    features.remove('NO2')\n",
        "\n",
        "    # create a list of the target\n",
        "    target = ['NO2']\n",
        "\n",
        "    # create a list of the train and test dataframes\n",
        "    train_df = model_df[model_df.lat_long.isin(train_lat_longs)]\n",
        "    test_df = model_df[model_df.lat_long.isin(test_lat_longs)].reset_index()\n",
        "    \n",
        "    # bootstrap model\n",
        "    model = BaggingRegressor(base_estimator=RandomForestRegressor(\n",
        "        n_estimators=10, max_depth=6, random_state=42), n_estimators=10, random_state=42)\n",
        "\n",
        "\n",
        "    # fit the model to the training data\n",
        "    model.fit(train_df[features], train_df[target].to_numpy().flatten())\n",
        "\n",
        "    # predict the test data\n",
        "    predictions = model.predict(test_df[features])\n",
        "\n",
        "    mse = mean_squared_error(test_df[test_df.Location == location].NO2,\n",
        "                       predictions[test_df[test_df.Location == location].index])\n",
        "    \n",
        "    mses[loc] = mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# create new_mses dict without the highest mse one\n",
        "new_mses = {k: v for k, v in mses.items() if v != max(mses.values())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = nice_plot(px.scatter_mapbox(lat=[x[0] for x in new_mses.keys()],\n",
        "                        lon=[x[1] for x in new_mses.keys()],\n",
        "                        color=[x for x in new_mses.values()],\n",
        "                        color_continuous_scale='RdYlGn_r',\n",
        "                        mapbox_style='carto-positron',\n",
        "                        zoom=11.5,\n",
        "), x_label=' ', y_label=' ', title='NO2 RMSE by Location')\n",
        "# change marker size\n",
        "fig.update_traces(marker=dict(size=30))\n",
        "# fig.write_image(file='MSE_for_each_location_Bristol_RF.pdf', format='pdf')\n",
        "fig.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### normal code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# create a list of the features\n",
        "features = ['hour', 'minute', \n",
        "            'tropospheric_NO2_column_number_density',\n",
        "            'NO2',\n",
        "            'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday',\n",
        "            'day', 'week',\n",
        "            'temperature_2m', 'relativehumidity_2m', 'windspeed_10m'\n",
        "            ]\n",
        "features.remove('NO2')\n",
        "\n",
        "# create a list of the target\n",
        "target = ['NO2']\n",
        "\n",
        "# create a list of the train and test dataframes\n",
        "train_df = model_df[model_df.lat_long.isin(train_lat_longs)]\n",
        "test_df = model_df[model_df.lat_long.isin(test_lat_longs)].reset_index()\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import mlp regressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# import functions for convolutional neural network\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, Dropout\n",
        "\n",
        "# import bagging regressor\n",
        "from sklearn.ensemble import BaggingRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # random forest regression model\n",
        "# model = RandomForestRegressor(n_estimators=10, max_depth=6, random_state=42)\n",
        "\n",
        "# # neural network regression model\n",
        "# model = MLPRegressor(hidden_layer_sizes=(100, 100, 100), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
        "\n",
        "# # neural network model\n",
        "# model = Sequential()\n",
        "# model.add(Dense(100, activation='relu', input_shape=(len(features),)))\n",
        "# model.add(Dense(100, activation='relu'))\n",
        "# model.add(Dense(100, activation='relu'))\n",
        "# model.add(Dense(1))\n",
        "# model.compile(optimizer='adam', loss='mse')\n",
        " \n",
        "\n",
        "# # convolutional neural network model\n",
        "# model = Sequential()\n",
        "# model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(len(features), 1)))\n",
        "# model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
        "# model.add(MaxPooling1D(pool_size=2))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(50, activation='relu'))\n",
        "# model.add(Dense(1))\n",
        "# model.compile(optimizer='adam', loss='mse')\n",
        " \n",
        "# bootstrap model\n",
        "model = BaggingRegressor(base_estimator=RandomForestRegressor(n_estimators=10, max_depth=6, random_state=42), n_estimators=10, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "# fit the model to the training data\n",
        "model.fit(train_df[features], train_df[target].to_numpy().flatten())\n",
        "\n",
        "# predict the test data\n",
        "predictions = model.predict(test_df[features])\n",
        "\n",
        "# calculate the mean squared error\n",
        "mse = mean_squared_error(test_df[target].to_numpy().flatten(), predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = np.array([x[0] for x in predictions])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the predictions and actual values for each location with plotly\n",
        "fig = nice_plot(go.Figure(), 'time', 'NO2')\n",
        "for location in test_df.Location.unique():\n",
        "    fig.add_trace(go.Scatter(x=test_df[test_df.Location == location].time, y=predictions[test_df[test_df.Location == location].index], mode='lines', name=f'{location} Predicted'))\n",
        "    fig.add_trace(go.Scatter(x=test_df[test_df.Location == location].time, y=test_df[test_df.Location == location].NO2, mode='lines', name=f'{location} Actual'))\n",
        "    # print mse for each location\n",
        "    print(f'{location} mse: {mean_squared_error(test_df[test_df.Location == location].NO2, predictions[test_df[test_df.Location == location].index])}')\n",
        "fig.update_layout(title='NO2 Measurements', xaxis_title='time', yaxis_title='NO2')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_6Sov3QuFir"
      },
      "source": [
        "### This is the loop that creates a dataframe with different variables in it (satellite, weather, time, date) and trains a model on that data\n",
        "\n",
        "- need to work on how data is split (by location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-tperubAIRI",
        "outputId": "aab5136a-153b-4726-ed10-733371c9e6eb"
      },
      "outputs": [],
      "source": [
        "values = []\n",
        "for lat, long in bris_combined.lat_long.unique():\n",
        "    # print(lat, long)\n",
        "    weather_url = f\"https://archive-api.open-meteo.com/v1/era5?latitude={lat}&longitude={long}&start_date={start}&end_date={end}&hourly=temperature_2m,relativehumidity_2m,windspeed_10m\"\n",
        "    response = rq.get(weather_url)\n",
        "    weather_json = response.json()\n",
        "    weather = pd.DataFrame.from_dict(weather_json['hourly'])\n",
        "    weather['time'] = pd.to_datetime(weather['time'], utc=True)\n",
        "\n",
        "# , 'day', 'week', 'hour', 'day_number'\n",
        "# 'relativehumidity_2m', 'temperature_2m', 'windspeed_10m',\n",
        "    model_df = pd.merge_asof(bris_combined[bris_combined.lat_long == (lat, long)].sort_values('time'), weather.sort_values('time'), on='time', direction='nearest', tolerance=pd.Timedelta('30min')).dropna(subset='NO2')\n",
        "\n",
        "    X = model_df[[\n",
        "        'tropospheric_NO2_column_number_density', # satellite\n",
        "        'day', 'week', 'day_number', # date\n",
        "        'hour', # time\n",
        "        'relativehumidity_2m', 'temperature_2m', 'windspeed_10m', # weather\n",
        "        ]]\n",
        "    y = model_df['NO2']\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    reg = LinearRegression().fit(X_train, y_train)\n",
        "    \n",
        "    predictions = reg.predict(X_test)\n",
        "    values.append(reg.score(X_test, y_test))\n",
        "    print(f\"{bris_combined[bris_combined.lat_long == (lat, long)].Location.iloc[0]} | {round(reg.score(X_test, y_test), 2)} | {[round(x, 2) for x in reg.coef_]}\")\n",
        "    \n",
        "# view linear regression coefficients\n",
        "\n",
        "print(f\"mean: {np.mean(values)}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "42u9YAXAzP7k"
      },
      "source": [
        "## Extra stuff that was used for plotting bristol data and initially getting london data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOYdDSu6AIRJ"
      },
      "outputs": [],
      "source": [
        "fig = nice_plot(px.scatter(bris_combined[bris_combined.Location != 'Marlborough Street'], x='NO2', y='tropospheric_NO2_column_number_density',\n",
        "                trendline='ols', opacity=0.2, color='Location'), 'Ground NO2 (μg/m³)', 'Satellite NO2 (mol/m²)')\n",
        "\n",
        "# fig.update_layout(legend=legend_dict2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LS9-A_8uAIRK",
        "outputId": "70c9eceb-b8e6-4264-b060-4a33499da610"
      },
      "outputs": [],
      "source": [
        "legend_dict2 = {}\n",
        "# get correlation coefficients for each location\n",
        "for lat_long in bris_combined.lat_long.unique():\n",
        "    location = bris_combined[bris_combined.lat_long ==lat_long].Location.iloc[0]\n",
        "    corr = bris_combined[bris_combined.Location == location][['NO2','tropospheric_NO2_column_number_density']].corr()\n",
        "    legend_dict2[lat_long] = f\"{location}: {round(corr.iloc[0, 1], 3)}\"\n",
        "    print(round(corr.NO2.iloc[1], 3), location)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaLTeUsXAIRL"
      },
      "outputs": [],
      "source": [
        "sat_aerosol['lat_long'] = sat_aerosol.lat_long.apply(lambda x: tuple(round(i, 3) for i in x))\n",
        "sat['lat_long'] = sat.lat_long.apply(lambda x: tuple(round(i, 3) for i in x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhfHilVLAIRM"
      },
      "outputs": [],
      "source": [
        "bristol_points = list(ground.lat_long.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z__lZryBAIRM"
      },
      "outputs": [],
      "source": [
        "reverse_bristol_points = [point[::-1] for point in bristol_points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziIbnF43AIRN"
      },
      "outputs": [],
      "source": [
        "ground['time2020'] = ground['time'].apply(lambda x: x.replace(year=2020))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PirIi3RPAIRN"
      },
      "outputs": [],
      "source": [
        "ground['date2020'] = ground['time2020'].apply(lambda x: x.date())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXxxDufJAIRO",
        "outputId": "605e5e05-0ece-418e-9f66-52e1f6338628"
      },
      "outputs": [],
      "source": [
        "fig = px.line(ground.sort_values('time2020').groupby(['Location', 'date2020']).mean().reset_index(), x='date2020', y='NO2', color='Location', title='NO2 levels in Bristol')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3SyP6UhAIRO"
      },
      "outputs": [],
      "source": [
        "ground['year'] = ground['time'].apply(lambda x: x.year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEY2eNCXAIRP"
      },
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = make_subplots(rows=1, cols=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmQ77kFrAIRP",
        "outputId": "1c17eb0f-0a9e-4747-c98f-872b8400830a"
      },
      "outputs": [],
      "source": [
        "# # plot average NO2 for each location over each day of the week\n",
        "fig1 = nice_plot(px.line(ground.dropna(subset='NO2').groupby(['Location', 'day_of_week']).mean().reset_index(), x='day_of_week', y='NO2', color='Location'), x_label='Day of week', y_label='', width=1000, y_range=[0, 140])\n",
        "\n",
        "# plot average NO2 for each location over each hour of the day\n",
        "fig2 = nice_plot(px.line(ground.dropna(subset='NO2').groupby(['Location', 'hour']).mean().reset_index(), x='hour', y='NO2', color='Location'), x_label='Hour of day', y_label='NO2 (μg/m³)', legend=False, y_range=[0, 140])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lJY08uDAIRR"
      },
      "outputs": [],
      "source": [
        "# # save figure\n",
        "# fig.write_image('plots/NO2_by_day_of_week.pdf')\n",
        "# fig.write_image('plots/NO2_by_hour.pdf')\n",
        "# fig.write_image('plots/NO2_over_year.pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBwYp_prAIRS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjRqfR3SAIRS",
        "outputId": "7dee8d0d-f6e6-4691-f213-f87c8ba2e8cb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y5b374tAIRT"
      },
      "outputs": [],
      "source": [
        "locs['lat_long'] = locs.Latitude.apply(lambda x: round(float(x), 3)).astype(str) + ',' + locs.Longitude.apply(lambda x: round(float(x), 3)).astype(str)\n",
        "locs['lat_long'] = locs.lat_long.apply(lambda x: tuple(map(float, x.split(','))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZmqEXuNAIRT"
      },
      "outputs": [],
      "source": [
        "ground.dropna(subset=['time'], inplace=True)\n",
        "ground.sort_values(by='time', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6lvVQ2NAIRT"
      },
      "outputs": [],
      "source": [
        "site_code = site_codes[0]\n",
        "start = '2018-07-11'\n",
        "end = '2022-10-31'\n",
        "end = '2018-07-13'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGjI-rmXAIRU"
      },
      "outputs": [],
      "source": [
        "test = pd.DataFrame.from_dict(response.json(), orient='index')\n",
        "data = test.Data.iloc[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMQGq9loAIRU"
      },
      "outputs": [],
      "source": [
        "# parse data from list of dictionaries\n",
        "data = pd.DataFrame.from_dict(data, orient='columns')\n",
        "# remove @ sign from column names\n",
        "data.columns = data.columns.str.replace('@', '')\n",
        "# pivot data so each species is a column\n",
        "data = data.pivot(index='MeasurementDateGMT', columns='SpeciesCode', values='Value').reset_index()\n",
        "data.index.name = 'index'\n",
        "data['Site_Code'] = site_code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjQ_XgWHAIRU",
        "outputId": "1e35345d-22e3-4e3e-97dd-4bc9c11870cb"
      },
      "outputs": [],
      "source": [
        "[x['@ColumnName'].split(': ')[-1] for x in response.json()['AirQualityData']['Columns']['Column']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXf2sC0kAIRU"
      },
      "outputs": [],
      "source": [
        "site_codes = locs.SiteCode.unique().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB35RpISAIRU"
      },
      "outputs": [],
      "source": [
        "locs.reset_index().to_feather('locations.feather')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsMnO38VAIRV"
      },
      "outputs": [],
      "source": [
        "### THIS ONE ###\n",
        "\n",
        "# start = '2018-07-11'\n",
        "start = '2020-07-11'\n",
        "end = '2022-10-11'\n",
        "# end = '2018-07-12'\n",
        "# df = pd.DataFrame()\n",
        "for site_code in tqdm(site_codes[-37:]):\n",
        "    url = f\"http://api.erg.ic.ac.uk/AirQuality/Data/Wide/Site/SiteCode={site_code}/StartDate={start}/EndDate={end}/Json\"\n",
        "    response = rq.get(url)\n",
        "    if response.status_code != 200:\n",
        "        continue\n",
        "    try:\n",
        "        data = pd.DataFrame.from_dict(response.json()['AirQualityData']['RawAQData']['Data'])\n",
        "        try:\n",
        "            cols = [x['@ColumnName'].split(': ')[-1]for x in response.json()['AirQualityData']['Columns']['Column']]\n",
        "        except:\n",
        "            cols = [response.json()['AirQualityData']['Columns']['Column']['@ColumnName'].split(': ')[-1]]\n",
        "        cols.insert(0, 'MeasurementDateGMT')\n",
        "        data.columns = cols\n",
        "        data['site_code'] = site_code\n",
        "        data['MeasurementDateGMT'] = pd.to_datetime(data['MeasurementDateGMT'])\n",
        "        df = pd.concat([df,data])\n",
        "        print(site_code)\n",
        "    except:\n",
        "        print(f'error,  {site_code}, {time.ctime()}')\n",
        "        \n",
        "df.reset_index().to_feather('london_data3.feather')\n",
        "print(f'Finished at {time.ctime()}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrupQb2eAIRV"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([pd.read_feather('london_data.feather'), pd.read_feather('london_data2.feather'), pd.read_feather('london_data3.feather')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFSUQvQQAIRV"
      },
      "outputs": [],
      "source": [
        "df.reset_index().to_feather('all_london_data.feather')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JIKkAnPAIRW"
      },
      "outputs": [],
      "source": [
        "df['Nitric Oxide (ug/m3)'] = pd.to_numeric(df['Nitric Oxide (ug/m3)'], downcast='float')\n",
        "df['Nitrogen Dioxide (ug/m3)'] = pd.to_numeric(df['Nitrogen Dioxide (ug/m3)'], downcast='float')\n",
        "df['Ozone (ug/m3)'] = pd.to_numeric(df['Ozone (ug/m3)'], downcast='float')\n",
        "df['PM10 Particulate (ug/m3)'] = pd.to_numeric(df['PM10 Particulate (ug/m3)'], downcast='float')\n",
        "df['PM2.5 Particulate (ug/m3)'] = pd.to_numeric(df['PM2.5 Particulate (ug/m3)'], downcast='float')\n",
        "df['Sulphur Dioxide (ug/m3)'] = pd.to_numeric(df['Sulphur Dioxide (ug/m3)'], downcast='float')\n",
        "df['Oxides of Nitrogen (ug/m3)'] = pd.to_numeric(df['Oxides of Nitrogen (ug/m3)'], downcast='float')\n",
        "df['site_code'] = df['site_code'].astype('category')\n",
        "df['Benzene (ug/m3)'] = pd.to_numeric(df['Benzene (ug/m3)'], downcast='float')\n",
        "df['Carbon Monoxide (mg/m3)'] = pd.to_numeric(df['Carbon Monoxide (mg/m3)'], downcast='float')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvIEkMjFAIRW"
      },
      "outputs": [],
      "source": [
        "df.drop(columns='Benzene (ug/m3)', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnFFHqESAIRW"
      },
      "outputs": [],
      "source": [
        "df['hour'] = df['MeasurementDateGMT'].dt.hour\n",
        "df['day'] = df['MeasurementDateGMT'].dt.day\n",
        "df['month'] = df['MeasurementDateGMT'].dt.month\n",
        "days = ['Monday', 'Tuesday', 'Wednesday',\n",
        "        'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "cat_type = CategoricalDtype(categories=days, ordered=True)\n",
        "df['day_of_week'] = df['MeasurementDateGMT'].dt.day_name().astype(cat_type)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SVgKjb2AIRW"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF2EJU_pAIRW"
      },
      "outputs": [],
      "source": [
        "# plot nitrogen dioxide levels for each site - with a red line\n",
        "fig1 = px.line(df.groupby('day_of_week').mean().reset_index(), x='day_of_week', y='Nitrogen Dioxide (ug/m3)', title='Nitrogen Dioxide Levels')\n",
        "# change the colour of the line\n",
        "fig1.update_traces(line_color='red')\n",
        "# add ground data\n",
        "fig2 = px.line(ground.groupby('day_of_week').mean().reset_index(), x='day_of_week', y='NO2')\n",
        "fig2.update_traces(line_color='blue')\n",
        "fig = nice_plot(go.Figure(data=fig1.data + fig2.data), 'Nitrogen Dioxide Levels', 'Day of Week', 'NO2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeXNa8xMAIRX"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter(x=ground.groupby('day_of_week').mean().reset_index()['day_of_week'], y=ground.groupby('day_of_week').mean().reset_index()['NO2'], name='Bristol'))\n",
        "fig.add_trace(go.Scatter(x=df.groupby('day_of_week').mean().reset_index()['day_of_week'], y=df.groupby(\n",
        "    'day_of_week').mean().reset_index()['Nitrogen Dioxide (ug/m3)'], name='London'))\n",
        "fig = nice_plot(fig, 'Nitrogen Dioxide Levels', 'Day of Week', 'NO2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWQppFfSAIRX"
      },
      "outputs": [],
      "source": [
        "# plot hourly average nitrogen dioxide levels for london and bristol\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=ground.groupby('hour').mean().reset_index()['hour'], y=ground.groupby('hour').mean().reset_index()['NO2'], name='Bristol'))\n",
        "fig.add_trace(go.Scatter(x=df.groupby('hour').mean().reset_index()['hour'], y=df.groupby('hour').mean().reset_index()['Nitrogen Dioxide (ug/m3)'], name='London'))\n",
        "fig = nice_plot(fig, 'Nitrogen Dioxide Levels', 'xth hour of the day', 'NO2')\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgsLhKjhAIRX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# pm25_lat_longs = list(ground[ground.Location.isin(pm25_locations)].lat_long.unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rac_zYOlAIRX"
      },
      "outputs": [],
      "source": [
        "# plot no2 lat longs and pm25 lat longs on shapefile\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scattermapbox(lat=[x[0] for x in no2_lat_longs], lon=[x[1] for x in no2_lat_longs], mode='markers', marker=go.scattermapbox.Marker(size=17), text=['NO2']*len(no2_lat_longs), name='NO2'))\n",
        "fig.add_trace(go.Scattermapbox(lat=[x[0] for x in pm25_lat_longs], lon=[x[1] for x in pm25_lat_longs], mode='markers', marker=go.scattermapbox.Marker(size=17), text=['PM2.5']*len(pm25_lat_longs), name='NO2 & PM2.5'))\n",
        "fig.update_layout(mapbox_style=\"carto-positron\", mapbox_zoom=11.7,\n",
        "                  mapbox_center={\"lat\": 51.454, \"lon\": -2.594})\n",
        "# change size of map\n",
        "fig.update_layout(height=600, margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0}, width=1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iupsfIQAIRY"
      },
      "outputs": [],
      "source": [
        "fig.write_image('plots/lon_bris_lat_long.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sYXEgqCAIRY"
      },
      "outputs": [],
      "source": [
        "df.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lk16H_sAIRY"
      },
      "outputs": [],
      "source": [
        "# df.to_feather('all_london_data.feather')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iem-FC2hAIRZ"
      },
      "outputs": [],
      "source": [
        "df.sort_values(by=['site_code', 'MeasurementDateGMT'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psq8DCAkAIRZ",
        "outputId": "9a08ceef-3f9b-45e8-ab34-c5b6fa37fe5b"
      },
      "outputs": [],
      "source": [
        "from get_satellite_data import get_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bristol.lat_long = bristol.lat_long.apply(lambda x: (x[0], x[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "london.lat_long = london.lat_long.apply(lambda x: (x[0], x[1]))\n",
        "london.lat_long.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set time to midnight, change year to 2000\n",
        "bristol['yearly_avg'] = bristol['time'].apply(lambda x: x.replace(year=2000, hour=0, minute=0, second=0, microsecond=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "london['yearly_avg'] = london['time'].apply(lambda x: x.replace(year=2000, hour=0, minute=0, second=0, microsecond=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bristol['week'] = bristol['yearly_avg'].dt.week\n",
        "london['week'] = london['yearly_avg'].dt.week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot weekly average nitrogen dioxide levels for 8 bristol lat_longs in red and 8 london lat_longs in blue\n",
        "fig = go.Figure()\n",
        "for n, i in enumerate(bristol.lat_long.unique()):\n",
        "    fig.add_trace(go.Scatter(x=bristol[bristol.lat_long == i].groupby('week').mean(numeric_only=True).reset_index()[\n",
        "                  'week'], y=bristol[bristol.lat_long == i].groupby('week').mean(numeric_only=True).reset_index()['NO2'], name=lat_long_to_location[i], line=dict(color='red')))\n",
        "for n, i in enumerate(np.random.choice(london.lat_long.unique(), 8)):\n",
        "    fig.add_trace(go.Scatter(x=london[london.lat_long == i].groupby('week').mean(numeric_only=True).reset_index()['week'], y=london[london.lat_long == i].groupby(\n",
        "        'week').mean(numeric_only=True).reset_index()['Nitrogen Dioxide (ug/m3)'], name=lat_long_to_location[i], line=dict(color='blue')))# \n",
        "fig = nice_plot(fig, 'Week of the year', 'NO2', width= 1600, height=400)\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig.write_image('plots/lon_bris_year.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "london_lat_long_to_location = dict(zip(locs.lat_long.unique(), locs.SiteName.unique()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bpefGybouqNp",
        "J3yxQ44zuyAd",
        "ou1KXLumq0GL",
        "lo7MI-uvt1jN",
        "h4cJAtL7uWH1",
        "n_6Sov3QuFir"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "858f3fce340b88cd1db1b15aba06637699134352860bd2dd99ba645e49113565"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
